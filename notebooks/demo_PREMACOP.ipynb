{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small demo of what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rnd_gen = np.random.default_rng(2024)\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 200 i.i.d examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "X = rnd_gen.normal(loc=0, scale=1, size=(N, 4))\n",
    "beta = np.array([2, 1, 0, 0])\n",
    "Y = X @ beta + rnd_gen.normal(loc=0, scale=1, size=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can become informative at step $\\lceil 2/\\epsilon \\rceil$, so for the first steps, ConformalRidgeRegressor will issue a warning if we try to predict anything before the training set is sufficiently large, and the prediction set will be infinite. However, we may as well learn the first few examples in one go, using .learn_inital_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_cp import ConformalRidgeRegressor\n",
    "\n",
    "cp = ConformalRidgeRegressor(rnd_state=2024)\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "cp.learn_initial_training_set(X[:int(np.ceil(2/epsilon))], Y[:int(np.ceil(2/epsilon))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current training set\n",
    "print(cp.X)\n",
    "print()\n",
    "print(cp.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are tuples with lower and upper bounds of the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = cp.predict(X[20], epsilon=epsilon)\n",
    "\n",
    "print(f'Prediction set: {Gamma}')\n",
    "\n",
    "print(f'Label: {Y[20]}')\n",
    "print(f'err: {cp.err(Gamma, Y[20])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want just upper or lower bounds, this can also be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma = cp.predict(X[20], epsilon=epsilon, bounds='upper')\n",
    "\n",
    "print(f'Prediction set: {Gamma}')\n",
    "\n",
    "print(f'Label: {Y[20]}')\n",
    "print(f'err: {cp.err(Gamma, Y[20])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the smoothed p-value of the object (but internally, for prediction, non-smoothed p-values are used), which can be used to test echangeablility using a test martingale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.compute_p_value(X[20], Y[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we learn the new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.learn_one(X[20], Y[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The full prediction Procedure\n",
    "Because predict, learn, and compute smoothed p-value have some computations in common, predict can return a \"precomputed\" object, that can be passed to the others to speed up calculations. \n",
    "\n",
    "When a new object is added, rather than recomputing from scratch, we use the Sherman Morrison formula to update online, which is a lot faster than adding a row and inverting the \"hat\" matrix.\n",
    "\n",
    "While we go along, the smoothed p-values can be used as an exchangeability check. We can set a warning level and rely on Ville's theorem. If ever the martingale grows beyond the warning level, the exchangeability hypothesis can be rejected at the corresponding confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_cp.martingale import PluginMartingale\n",
    "\n",
    "cp = ConformalRidgeRegressor()\n",
    "martingale = PluginMartingale(warning_level=100)\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "Err = 0 \n",
    "\n",
    "cp.learn_initial_training_set(X[:int(np.ceil(2/epsilon))], Y[:int(np.ceil(2/epsilon))])\n",
    "\n",
    "res = np.zeros(shape=(N-20, 7))\n",
    "\n",
    "for i, (object, label) in tqdm(enumerate(zip(X[int(np.ceil(2/epsilon)):], Y[int(np.ceil(2/epsilon)):])), total=N-20, desc='Running online conformal prediction'):\n",
    "    # Reality outputs object\n",
    "    x = object\n",
    "\n",
    "    # Forecaster outputs a prediction set \n",
    "    Gamma, precomputed = cp.predict(x=x, epsilon=epsilon, bounds='both', return_update=True) # We return the precomputed update for later use\n",
    "\n",
    "    # Reality outputs label\n",
    "    y = label\n",
    "\n",
    "    # Observe error\n",
    "    err = cp.err(Gamma=Gamma, y=y)\n",
    "    Err += err\n",
    "\n",
    "    # Learn new object\n",
    "    cp.learn_one(x=x, y=y, precomputed=precomputed) # We pass precomputed as an argument to avoid redundant computations\n",
    "\n",
    "    # Compute smoothed p-value\n",
    "    p = cp.compute_p_value(x=x, y=y, precomputed=precomputed) # We pass precomputed as an argument to avoid redundant computations\n",
    "\n",
    "    # Update martingale\n",
    "    martingale.update_martingale_value(p)\n",
    "\n",
    "    res[i, 0] = Gamma.lower\n",
    "    res[i, 1] = Gamma.upper\n",
    "    res[i, 2] = label\n",
    "    res[i, 3] = err\n",
    "    res[i, 4] = Err\n",
    "    res[i, 5] = cp.width(Gamma) # Simple efficiency criterion for interval predictions\n",
    "    res[i, 6] = martingale.logM\n",
    "\n",
    "print(f'Error rate: {Err/(N-20)}')\n",
    "print(f'Maximum martingale value: {martingale.max}')\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "axs[0,0].plot(res[:, 2], label='Truth')\n",
    "axs[0,0].fill_between(\n",
    "    x=[i for i in range(N-20)],\n",
    "    y1=res[:, 0],\n",
    "    y2=res[:, 1],\n",
    "    color='C2',\n",
    "    alpha=0.7,\n",
    "    label='Prediction'\n",
    ")\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].plot(res[:,6], label='Log Martingale')\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1,0].plot(res[:, 4], label='Cummulative error')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].plot(res[:, 5], label='Width')\n",
    "axs[1,1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test it again for non-exchangeable data\n",
    "Now we introduce two change points, but keep the same objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = np.array([2, 1, 0, 0])\n",
    "beta2 = np.array([0, 0, 100, 20])\n",
    "\n",
    "Y1 = X[:int(N/2)] @ beta1 + rnd_gen.normal(loc=0, scale=1, size=int(N/2))\n",
    "Y2 = X[int(N/2):N] @ beta2 + rnd_gen.normal(loc=0, scale=1, size=int(N/2))\n",
    "Y = np.concatenate([Y1, Y2]) # The test martingale should raise a warning shortly after n = 501."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat irritating to have the martingale warn throughout (which will happen), so user warnings can be turned off by passing an argument in the construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ConformalRidgeRegressor(rnd_state=2024)\n",
    "martingale = PluginMartingale(warning_level=100, warnings=False) # Disable user warnings because we check in the end\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "Err = 0 \n",
    "\n",
    "cp.learn_initial_training_set(X[:int(np.ceil(2/epsilon))], Y[:int(np.ceil(2/epsilon))])\n",
    "\n",
    "res = np.zeros(shape=(N-20, 7))\n",
    "\n",
    "warned = False # Identify first step where warning is raised\n",
    "\n",
    "for i, (object, label) in tqdm(enumerate(zip(X[int(np.ceil(2/epsilon)):], Y[int(np.ceil(2/epsilon)):])), total=N-20, desc='Running online conformal prediction'):\n",
    "    # Reality outputs object\n",
    "    x = object\n",
    "\n",
    "    # Forecaster outputs a prediction set \n",
    "    Gamma, precomputed = cp.predict(x=x, epsilon=epsilon, bounds='both', return_update=True) # We return the precomputed update for later use\n",
    "\n",
    "    # Reality outputs label\n",
    "    y = label\n",
    "\n",
    "    # Observe error\n",
    "    err = cp.err(Gamma=Gamma, y=y)\n",
    "    Err += err\n",
    "\n",
    "    # Learn new object\n",
    "    cp.learn_one(x=x, y=y, precomputed=precomputed) # We pass precomputed as an argument to avoid redundant computations\n",
    "\n",
    "    # Compute smoothed p-value\n",
    "    p = cp.compute_p_value(x=x, y=y, precomputed=precomputed) # We pass precomputed as an argument to avoid redundant computations\n",
    "\n",
    "    # Update martingale\n",
    "    martingale.update_martingale_value(p)\n",
    "\n",
    "    if martingale.max >= 100 and warned==False:\n",
    "        warning_step = i # To identify the first time a warning was raised\n",
    "        warned = True\n",
    "\n",
    "    res[i, 0] = Gamma.lower\n",
    "    res[i, 1] = Gamma.upper\n",
    "    res[i, 2] = label\n",
    "    res[i, 3] = err\n",
    "    res[i, 4] = Err\n",
    "    res[i, 5] = cp.width(Gamma)\n",
    "    res[i, 6] = martingale.logM\n",
    "\n",
    "print(f'Error rate: {Err/(N-20)}')\n",
    "print(f'Maximum martingale value: {martingale.max}')\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "axs[0,0].plot(res[:, 2], label='Truth')\n",
    "axs[0,0].fill_between(\n",
    "    x=[i for i in range(N-20)],\n",
    "    y1=res[:, 0],\n",
    "    y2=res[:, 1],\n",
    "    color='C2',\n",
    "    alpha=0.7,\n",
    "    label='Prediction'\n",
    ")\n",
    "axs[0,0].axvline(warning_step, color='red', label='Warning raised', linestyle='dashed')\n",
    "axs[0,0].axvline(int(N/2), color='C1', label='Change point', linestyle='dashed')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].plot(res[:,6], label='Log Martingale')\n",
    "axs[0,1].axvline(warning_step, color='red', label='Warning raised', linestyle='dashed')\n",
    "axs[0,1].axvline(int(N/2), color='C1', label='Change point', linestyle='dashed')\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1,0].plot(res[:, 4], label='Cummulative error')\n",
    "axs[1,0].axvline(warning_step, color='red', label='Warning raised', linestyle='dashed')\n",
    "axs[1,0].axvline(int(N/2), color='C1', label='Change point', linestyle='dashed')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].plot(res[:, 5], label='Width')\n",
    "axs[1,1].axvline(warning_step, color='red', label='Warning raised', linestyle='dashed')\n",
    "axs[1,1].axvline(int(N/2), color='C1', label='Change point', linestyle='dashed')\n",
    "axs[1,1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A warning was raised about 100 steps after the change point. Efficiency of conformal test martingales is an apen question, but the difficulty of detection depends on many things, including the nature of the change, and its magnitude. Plugin martingale approximates the optimal betting strategy, which is the likelihood ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We also have the nearest neighbours classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "N = 200\n",
    "\n",
    "X, Y = make_classification(n_samples=N, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_classes=3, n_clusters_per_class=1, random_state=2024)\n",
    "\n",
    "\n",
    "# Create a scatter plot with different symbols for each class\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define different markers for each class\n",
    "markers = ['o', 's', 'D']\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "# Plot each class with a different marker\n",
    "for label, marker, color in zip(np.unique(Y), markers, colors):\n",
    "    plt.scatter(X[Y == label, 0], X[Y == label, 1], label=f'Class {label}', marker=marker, color=color, edgecolor='k')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.title('Synthetic Classification Dataset with 3 Classes')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal nearest neighbours classifier\n",
    "Nonconformity score for the one nearest neighbour classifier is the ratio \n",
    "\n",
    "$\\frac{\\text{Distance to nearest neighbour with the same label}}{\\text{Distance to nearest neighbour with a different label}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_cp.classifiers import ConformalNearestNeighboursClassifier\n",
    "\n",
    "cp = ConformalNearestNeighboursClassifier(k=1, label_space=np.unique(Y))\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "cp.learn_initial_training_set(X[:int(np.ceil(1/epsilon))], Y[:int(np.ceil(1/epsilon))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, prediction set is a list of possible labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma, p_values = cp.predict(X[10], epsilon=epsilon, return_p_values=True)\n",
    "\n",
    "print(f'Prediction set: {Gamma}')\n",
    "print(f'p-values: {p_values}')\n",
    "print(f'Label: {Y[10]}')\n",
    "print(f'err: {cp.err(Gamma, Y[10])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run full example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "k = 1\n",
    "cp = ConformalNearestNeighboursClassifier(k=k, label_space=np.unique(Y))\n",
    "\n",
    "martingale = PluginMartingale(warning_level=100) # Disable user warnings because we check in the end\n",
    "\n",
    "res = np.zeros(shape=(Y.shape[0], 4))\n",
    "\n",
    "for i, (object, label) in tqdm(enumerate(zip(X, Y)), total=Y.shape[0], desc='Running online conformal prediction'):\n",
    "\n",
    "    # Reality outputs object\n",
    "    x = object\n",
    "\n",
    "    # Forecaster outputs prediction set\n",
    "    Gamma, p_values, D = cp.predict(x, epsilon=epsilon, return_p_values=True, return_update=True)\n",
    "\n",
    "    # Reality outputs label\n",
    "    y = label\n",
    "\n",
    "    # Check error\n",
    "    cp.err(Gamma, label)\n",
    "\n",
    "    # Learn the label\n",
    "    cp.learn_one(object, y, D)\n",
    "    \n",
    "    # Prefferred efficiency criteria\n",
    "\n",
    "    # Observed excess\n",
    "    cp.oe(Gamma, y)\n",
    "\n",
    "    # Observed fuzziness\n",
    "    cp.of(p_values, y)\n",
    "\n",
    "    # Update martingale\n",
    "    martingale.update_martingale_value(p_values[y])\n",
    "\n",
    "\n",
    "    res[i, 0] = cp.OE\n",
    "    res[i, 1] = cp.OF\n",
    "    res[i, 2] = cp.Err\n",
    "    res[i, 3] = martingale.logM\n",
    "\n",
    "print(f'Average error: {cp.Err/Y.shape[0]}')\n",
    "print(f'Average observed excess: {cp.OE/Y.shape[0]}')\n",
    "print(f'Average observed fuzziness: {cp.OF/Y.shape[0]}')\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "axs[0,0].plot(res[:, 2], label='Cummulative error')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].plot(res[:,3], label='Log Martingale')\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1,0].plot(res[:, 0], label='Cummulative OE')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].plot(res[:, 1], label='Cummulative OF')\n",
    "axs[1,1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future considerations\n",
    "\n",
    "There are some other conformal predictors implemented, including nearest neighbours and kernel ridge regression (with several kernels), as well as a (possibly correct) nearest neighbours conformal predictite system. \n",
    "\n",
    "### Release minimal version \n",
    "For use in projects, it may be good to have a released minimal version of OnlineConformalPrediction. Initially, it could include\n",
    "* Conformalised Ridge Regression\n",
    "* Plugin martingale\n",
    "* Possibly Conformalised Nearest Neighbours Regression (but I will have to check it carefully for any bugs)\n",
    "* Conformal Nearest Neighbours Classification\n",
    "\n",
    "I don't know if such a minimal version should be acompanied by a paper, or if that can wait. Feels like a Copa paper for next year.\n",
    "\n",
    "### Linear regression\n",
    "We will initally focus on regression, but online classification is actually easier. A simple class that uses e.g. scikit-learn classifiers to define nonconformity measure could be easily implemented. \n",
    "\n",
    "There are at least three commonly used regularisations used in linear regression, all of which are compatible with the kernel trick. \n",
    "* $L1$ (Lasso)\n",
    "* $L2$ (Ridge)\n",
    "* Linear combination of the above (Elastic net)\n",
    "\n",
    "All of these can be conformalized, and at least Ridge can also be used in conformal predictive systems (CPS).\n",
    "\n",
    "There is also the Studentized for of ridge regression, and Bayesian ridge regression to consider.\n",
    "\n",
    "Another relatively simple regressor is the k-nearest neighbours algorithm, which is very flexible as it can use arbitrary distances. It is particularly interesting in the CPS setting. The distance can be measured in feature space as defined by a kernel.\n",
    "\n",
    "Ridge and KNN are described in detail in Algorithmic Learning in a Random World. Lasso and Elastic net are conformalised in the paper Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy, but I am unaware of any extention to CPS. \n",
    "\n",
    "### Classification\n",
    "* Support Vector Machines (SVM) can be conformalised, and should probably be included"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlineCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
