{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from online_cp import ConformalRidgeRegressor\n",
    "from online_cp.CPS import RidgePredictionMachine\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from online_cp.evaluation import Evaluation, Err, OF, OE, WinklerScore, Width, CRPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 250\n",
    "\n",
    "X, Y = make_classification(n_samples=N, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_classes=3, n_clusters_per_class=1, random_state=2024)\n",
    "\n",
    "\n",
    "# Create a scatter plot with different symbols for each class\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define different markers for each class\n",
    "markers = ['o', 's', 'D']\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "# Plot each class with a different marker\n",
    "for label, marker, color in zip(np.unique(Y), markers, colors):\n",
    "    plt.scatter(X[Y == label, 0], X[Y == label, 1], label=f'Class {label}', marker=marker, color=color, edgecolor='k')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.title('Synthetic Classification Dataset with 3 Classes')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from online_cp import ConformalNearestNeighboursClassifier\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "init_train = int(np.ceil(1/epsilon))\n",
    "X_train = X[:init_train]\n",
    "y_train = Y[:init_train]\n",
    "X_run = X[init_train:]\n",
    "y_run = Y[init_train:]\n",
    "\n",
    "cp = ConformalNearestNeighboursClassifier(k=1, label_space=np.unique(Y))\n",
    "\n",
    "efficiency = Evaluation(err=Err, oe=OE, of=OF)\n",
    "\n",
    "cp.learn_initial_training_set(X=X_train, y=y_train)\n",
    "\n",
    "for x, y in zip(X_run, y_run):\n",
    "    \n",
    "    # Make prediction\n",
    "    Gamma, p_values, D = cp.predict(x, epsilon=0.1, return_p_values=True, return_update=True) \n",
    "\n",
    "    # Learn the label\n",
    "    cp.learn_one(x, y, D)\n",
    "\n",
    "    # Update efficiency criteria\n",
    "    efficiency.update(y=y, Gamma=Gamma, p_values=p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.plot_cumulative_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "N = 300\n",
    "data = housing.data\n",
    "target = housing.target\n",
    "\n",
    "X, X_other, Y, Y_other = train_test_split(data, target, train_size=N, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ConformalRidgeRegressor(studentised=True)\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "# Ensure that we can get informative prediction sets\n",
    "X_init_train = X[:int(np.ceil(2/epsilon))]\n",
    "y_init_train = Y[:int(np.ceil(2/epsilon))]\n",
    "\n",
    "X_process = X[int(np.ceil(2/epsilon)):]\n",
    "y_process = Y[int(np.ceil(2/epsilon)):]\n",
    "\n",
    "cp.learn_initial_training_set(X_init_train, y_init_train)\n",
    "\n",
    "efficiency = Evaluation(err=Err, winkler=WinklerScore, width=Width)\n",
    "\n",
    "for x, y in zip(X_process, y_process):\n",
    "    \n",
    "    # Make prediction\n",
    "    Gamma, precomputed = cp.predict(x, epsilon=0.1, return_update=True) \n",
    "    # To avoid repeating computations, we return some precomputed arrays if return_update=True\n",
    "\n",
    "    # Learn the label\n",
    "    cp.learn_one(x, y, precomputed)\n",
    "    # We do not have to invert a matrix at each step n. The hat matrix can be efficiently updaten online using the Sherman-Morrison formula\n",
    "\n",
    "    # Update efficiency criteria\n",
    "    efficiency.update(y=y, Gamma=Gamma, epsilon=epsilon)\n",
    "\n",
    "precomputed_cp_for_later = precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.plot_cumulative_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps = RidgePredictionMachine()\n",
    "\n",
    "cps.learn_initial_training_set(X_init_train, y_init_train)\n",
    "\n",
    "efficiency = Evaluation(err=Err, winkler=WinklerScore, width=Width, crps=CRPS)\n",
    "\n",
    "for x, y in zip(X_process, y_process):\n",
    "    tau = np.random.uniform(0, 1)\n",
    "    \n",
    "    # Compute CPD\n",
    "    cpd, precomputed = cps.predict_cpd(x, return_update=True) \n",
    "    # To avoid repeating computations, we return some precomputed arrays if return_update=True\n",
    "\n",
    "    Gamma = cpd.predict_set(tau=tau, epsilon=epsilon, minimise_width=True)\n",
    "\n",
    "    # Learn the label\n",
    "    cps.learn_one(x, y, precomputed)\n",
    "    # We do not have to invert a matrix at each step n. The hat matrix can be efficiently updaten online using the Sherman-Morrison formula\n",
    "\n",
    "    # Update efficiency\n",
    "    efficiency.update(y=y, Gamma=Gamma, epsilon=epsilon, cpd=cpd)\n",
    "\n",
    "    # Compute p-value\n",
    "    p = cpd(y, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.plot_cumulative_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are there analogous criteria in regression?\n",
    "\n",
    "* The OE criterion is the size of the set of p-values larger than $\\varepsilon$. If we have a cpd, that would be $|\\{p(y): p(y)\\neq p(y_i), p(y) > \\varepsilon\\}| = |\\{p(y):p(y)>\\varepsilon\\}|$. This could be computed as the integral of $\\Pi$ from the $\\varepsilon$-quantile to $\\sup\\{y:\\Pi(y,\\tau)\\leq 1\\}$, but unfortunately, that does not converge.\n",
    "* The E criterion in regression is just the M criterion unless $\\Gamma=\\emptyset$.\n",
    "* We don't really compute p-values in regression, but we could, for an interval compute the p-values for the lower and upper bounds of the prediction interval. All $y\\in\\mathbb{R}\\backslash\\Gamma$ have smaller p-values.\n",
    "\n",
    "# FILL IN LATER, BUT DO IT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are some interesting integrals to consider in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of the Studentized CLS p-value for varying $y$-values (for $x_n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda y: cp.compute_p_value(x, y, precomputed=precomputed_cp_for_later, smoothed=False)\n",
    "\n",
    "yrange = np.linspace(-4, 8, 100)\n",
    "plt.plot(yrange, [func(y) for y in yrange])\n",
    "plt.axvline(func(y), color='red', linestyle='--', label=r'$p(y_n)$')\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$p(y)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know (almost surely), that under natural assumptions, the prediction interval is finite for $\\varepsilon \\geq \\frac{2}{n}$, and infinite otherwise. Thus, the integral from the corresponding lower to the corresponding upper value, may be seen as some kind of $\\varepsilon$-free width criterion. It is a little tricky to compute, but in principle it should be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = cp.predict(x, epsilon=2/cp.X.shape[0])\n",
    "finite_yrange = np.linspace(interval.lower, interval.upper, endpoint=True, num=1000)\n",
    "plt.plot(finite_yrange, [func(y) for y in finite_yrange])\n",
    "plt.fill_between(finite_yrange, 0, [func(y) for y in finite_yrange], color='green', alpha=0.5, label=r'$\\int p(y)dy$')\n",
    "plt.xlabel(r'$y$')\n",
    "plt.ylabel(r'$p(y)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The width criterion for CPD is $\\varepsilon$-dependent\n",
    "But the dependence is empirically small. However, if we want to get rid of it, we could integrate the width over $\\varepsilon$. Again there may be issues, since there will be values of $\\varepsilon$ that give infinite intervals. At least in principle, we could consider\n",
    "\n",
    "$\\int_{\\varepsilon^*}^1\\text{W}(\\Gamma^{\\varepsilon})d\\varepsilon$,\n",
    "where $\\varepsilon^* = \\inf\\{\\varepsilon : |\\Gamma^{\\varepsilon}|<\\infty\\}$ to be an efficiency criterion. Perhaps we might consider the number $(1-\\varepsilon^*)\\int_{\\varepsilon^*}^1\\text{W}(\\Gamma^{\\varepsilon})d\\varepsilon$. It will typically be computationally nasty to compute.\n",
    "\n",
    "A compromise would be to average the with over several $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.random.uniform(0, 1)\n",
    "func = lambda epsilon: cpd.predict_set(tau=tau, epsilon=epsilon).width()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, 1000, endpoint=True), [func(epsilon) for epsilon in np.linspace(0, 1, 1000, endpoint=True)])\n",
    "plt.fill_between(np.linspace(0, 1, 1000, endpoint=True), [func(epsilon) for epsilon in np.linspace(0, 1, 1000, endpoint=True)], alpha=0.5, label=r'$\\int W(\\varepsilon)d\\varepsilon$')\n",
    "plt.xlabel(r'$\\epsilon$')\n",
    "plt.ylabel(r'Width')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlineCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
