{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from online_cp import PluginMartingale\n",
    "from online_cp import NearestNeighboursPredictionMachine, RidgePredictionMachine\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "rnd_gen = np.random.default_rng(2025)\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "MACHINE_EPSILON = lambda x: np.abs(x) * np.finfo(np.float64).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to figure out normalisation. K-nn is not all that good without it...\n",
    "\n",
    "It would also be very nice to figure out how to make it Mondrian...\n",
    "\n",
    "Easiest should be to normalise the data as we go along, and use learn_initial_training_set each time. It will slow down things a bit, but it would work in principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('export.csv', parse_dates=True, index_col='Time')\n",
    "data = data.asfreq('h')\n",
    "data = data.dropna()\n",
    "\n",
    "Winter_train = data[:4308]\n",
    "Winter_test = data[4308:]\n",
    "\n",
    "D = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_training_set_size = D + Winter_train.shape[0]\n",
    "knonw_initially = data[:initial_training_set_size]\n",
    "future = data[initial_training_set_size:]\n",
    "\n",
    "X_initial_train = knonw_initially[:-D]\n",
    "y_initial_train = knonw_initially[D:].demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT\n"
     ]
    }
   ],
   "source": [
    "learn_cols = ['demand', f'temperature_{D}', f'Hour_{D}', f'Working_day_{D}']\n",
    "pred_cols = ['demand', f'temperature_forecast', f'Hour_{D}', f'Working_day_{D}']\n",
    "\n",
    "X_train = X_initial_train[learn_cols]\n",
    "y_train = y_initial_train\n",
    "\n",
    "# cps = NearestNeighboursPredictionMachine(k=neighbours)\n",
    "cps = RidgePredictionMachine(autotune=True)\n",
    "cps.learn_initial_training_set(\n",
    "    X=X_train[learn_cols].values,\n",
    "    y=y_train.values\n",
    ")\n",
    "cpd = cps.predict_cpd(X_initial_train.iloc[-1][pred_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = cpd\n",
    "\n",
    "# self.y_vals = np.array(sorted([-np.inf, np.inf] + self.C[1: -1].tolist() + (self.C[1: -1] + MACHINE_EPSILON(self.C[1: -1])).tolist() + (self.C[1: -1] - MACHINE_EPSILON(self.C[1: -1])).tolist()))\n",
    "# self.L = np.array([self.__call__(y, 0) for y in self.y_vals])\n",
    "# self.U = np.array([self.__call__(y, 1) for y in self.y_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 1\n",
    "tau = 0.442142334\n",
    "self.Y[np.where((1 - tau) * self.L + tau * self.U >= p)[0].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    self.quantile(p, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = False\n",
    "\n",
    "neighbours = 101\n",
    "\n",
    "autotune=True\n",
    "\n",
    "normalisation = False\n",
    "\n",
    "pred_cols = ['demand', f'temperature_forecast', f'Hour_{D}', f'Working_day_{D}']\n",
    "learn_cols = ['demand', f'temperature_{D}', f'Hour_{D}', f'Working_day_{D}']\n",
    "Hours = data.index.hour.unique().to_numpy()\n",
    "\n",
    "if normalisation:\n",
    "\n",
    "    if hourly:\n",
    "        X_train_dict = {hour: X_initial_train.loc[X_initial_train.index.hour==hour][learn_cols] for hour in Hours}\n",
    "        y_train_dict = {hour: y_initial_train.loc[X_initial_train.index.hour==hour].values for hour in Hours}\n",
    "\n",
    "        # hourly_CPS = {hour: NearestNeighboursPredictionMachine(k=neighbours) for hour in Hours}\n",
    "        hourly_CPS = {hour: RidgePredictionMachine(autotune=autotune) for hour in Hours}\n",
    "\n",
    "        [hourly_CPS[hour].learn_initial_training_set(\n",
    "                X=minmax_scale(X_train_dict[hour]),\n",
    "                y=y_train_dict[hour]\n",
    "                ) for hour in Hours]\n",
    "\n",
    "    else:\n",
    "        X_train = X_initial_train[learn_cols]\n",
    "        y_train = y_initial_train\n",
    "\n",
    "        # cps = NearestNeighboursPredictionMachine(k=neighbours)\n",
    "        cps = RidgePredictionMachine(autotune=autotune)\n",
    "        cps.learn_initial_training_set(\n",
    "            X=minmax_scale(X_train),\n",
    "            y=y_train.values\n",
    "        )\n",
    "\n",
    "    martingale = PluginMartingale(warnings=False, window_size='adaptive')\n",
    "    calibration_martingale = PluginMartingale(warnings=False)\n",
    "\n",
    "    res = np.zeros(shape=(future.shape[0]-D, 4))\n",
    "    p_values = np.zeros(shape=(future.shape[0]-D,))\n",
    "    protected_p_values = np.zeros(shape=(future.shape[0]-D,))\n",
    "    p_values_cat = {i: [] for i in Hours}\n",
    "    protected_p_values_cat = {i: [] for i in Hours}\n",
    "\n",
    "    dists = {}\n",
    "    protected_dists = {}\n",
    "\n",
    "    limit = 40\n",
    "\n",
    "    j = 0\n",
    "    for i, obj in tqdm(future.iterrows(), total=future.shape[0], desc=f'Decision protocol {D}h (normalised data)'):\n",
    "        tau = rnd_gen.uniform(0, 1)\n",
    "\n",
    "        # This is all we know now\n",
    "        known = data[data.index <= i]\n",
    "\n",
    "        if hourly:\n",
    "            cps = hourly_CPS[i.hour]   \n",
    "            X_train = X_train_dict[i.hour]\n",
    "            y_train = y_train_dict[i.hour]\n",
    "\n",
    "        # Reality outputs object\n",
    "        x = obj[pred_cols].values\n",
    "\n",
    "        # And we scale it\n",
    "        X_scale = np.vstack([X_train, x])\n",
    "        x = minmax_scale(X_scale)[-1]\n",
    "\n",
    "        # Forecaster outputs conformal predictive distribution\n",
    "        cpd = cps.predict_cpd(x=x)\n",
    "\n",
    "        protected_cpd = lambda y, tau: martingale.B_n(cpd(y, tau)) # FIXME This object changes after it has been saved...\n",
    "\n",
    "        pred = cpd(limit, tau)\n",
    "        protected_pred = protected_cpd(limit, tau)\n",
    "\n",
    "        # Reality outputs label (the demand now) for the object x_{t-24}\n",
    "        y = obj.demand\n",
    "        x_minus_24 = known.loc[i - pd.Timedelta(D, 'h')][learn_cols].values\n",
    "        # And we scale it\n",
    "        X_train = np.vstack([X_train, x_minus_24])\n",
    "        y_train = np.append(y_train, y)\n",
    "        x_minus_24 = minmax_scale(X_scale)[-1]\n",
    "\n",
    "        # Learn new example (relearn everything)\n",
    "        cat_learn = (i - pd.Timedelta(D, 'h')).hour\n",
    "        if hourly:\n",
    "            hourly_CPS[cat_learn].learn_initial_training_set(X=minmax_scale(X_train), y=y_train)\n",
    "        else:\n",
    "            cps.learn_initial_training_set(X=minmax_scale(X_train), y=y_train)\n",
    "\n",
    "        # hourly_CPS[cat_learn].learn_one(x_minus_24, y)\n",
    "        # cps.learn_one(x_minus_24, y)\n",
    "\n",
    "        if j < future.shape[0] - 24:\n",
    "            y_plus_24 = future.loc[i + pd.Timedelta(D, 'h')].demand\n",
    "            res[j, 0] = pred\n",
    "            res[j, 1] = protected_pred\n",
    "            res[j, 2] = y_plus_24\n",
    "            res[j, 3] = tau\n",
    "            dists[j] = cpd\n",
    "            protected_dists[j] = deepcopy(protected_cpd)\n",
    "\n",
    "            p = cpd(y_plus_24, tau)\n",
    "            protected_p = protected_cpd(y_plus_24, tau)\n",
    "            p_values[j] = p\n",
    "            p_values_cat[cat_learn].append(p)\n",
    "            protected_p_values[j] = protected_p\n",
    "            protected_p_values_cat[cat_learn].append(protected_p)\n",
    "            martingale.update_martingale_value(p)\n",
    "            calibration_martingale.update_martingale_value(protected_p)\n",
    "        j += 1\n",
    "\n",
    "else:\n",
    "\n",
    "    if hourly:\n",
    "        # hourly_CPS = {hour: NearestNeighboursPredictionMachine(k=neighbours) for hour in Hours}\n",
    "        hourly_CPS = {hour: RidgePredictionMachine(autotune=autotune) for hour in Hours}\n",
    "\n",
    "        [hourly_CPS[hour].learn_initial_training_set(\n",
    "                X=X_initial_train.loc[X_initial_train.index.hour==hour][learn_cols].values,\n",
    "                y=y_initial_train.loc[X_initial_train.index.hour==hour].values\n",
    "                ) for hour in Hours]\n",
    "    else:\n",
    "        # cps = NearestNeighboursPredictionMachine(k=neighbours)\n",
    "        cps = RidgePredictionMachine(autotune=autotune)\n",
    "        cps.learn_initial_training_set(\n",
    "            X=X_initial_train[learn_cols].values,\n",
    "            y=y_initial_train.values\n",
    "        )\n",
    "\n",
    "    martingale = PluginMartingale(warnings=False, window_size='adaptive')\n",
    "    calibration_martingale = PluginMartingale(warnings=False)\n",
    "\n",
    "    res = np.zeros(shape=(future.shape[0]-D, 4))\n",
    "    p_values = np.zeros(shape=(future.shape[0]-D,))\n",
    "    protected_p_values = np.zeros(shape=(future.shape[0]-D,))\n",
    "    p_values_cat = {i: [] for i in Hours}\n",
    "    protected_p_values_cat = {i: [] for i in Hours}\n",
    "\n",
    "    dists = {}\n",
    "    protected_dists = {}\n",
    "\n",
    "    limit = 40\n",
    "\n",
    "    j = 0\n",
    "    for i, obj in tqdm(future.iterrows(), total=future.shape[0], desc=f'Decision protocol {D}h (raw data)'):\n",
    "        tau = rnd_gen.uniform(0, 1)\n",
    "\n",
    "        # This is all we know now\n",
    "        known = data[data.index <= i]\n",
    "\n",
    "        if hourly:\n",
    "            cps = hourly_CPS[i.hour]   \n",
    "\n",
    "        # Reality outputs object\n",
    "        x = obj[pred_cols].values\n",
    "\n",
    "        # Forecaster outputs conformal predictive distribution\n",
    "        cpd = cps.predict_cpd(x=x)\n",
    "\n",
    "        protected_cpd = lambda y, tau: martingale.B_n(cpd(y, tau)) # FIXME This object changes after it has been saved...\n",
    "\n",
    "        pred = cpd(limit, tau)\n",
    "        protected_pred = protected_cpd(limit, tau)\n",
    "\n",
    "        # Reality outputs label (the demand now) for the object x_{t-24}\n",
    "        y = obj.demand\n",
    "        x_minus_24 = known.loc[i - pd.Timedelta(D, 'h')][learn_cols].values\n",
    "\n",
    "        # Learn new example\n",
    "        cat_learn = (i - pd.Timedelta(D, 'h')).hour\n",
    "        if hourly:\n",
    "            hourly_CPS[cat_learn].learn_one(x_minus_24, y)\n",
    "        else:\n",
    "            cps.learn_one(x_minus_24, y)\n",
    "\n",
    "        if j < future.shape[0] - 24:\n",
    "            y_plus_24 = future.loc[i + pd.Timedelta(D, 'h')].demand\n",
    "            res[j, 0] = pred\n",
    "            res[j, 1] = protected_pred\n",
    "            res[j, 2] = y_plus_24\n",
    "            res[j, 3] = tau\n",
    "            dists[j] = cpd\n",
    "            protected_dists[j] = deepcopy(protected_cpd)\n",
    "\n",
    "            p = cpd(y_plus_24, tau)\n",
    "            protected_p = protected_cpd(y_plus_24, tau)\n",
    "            p_values[j] = p\n",
    "            p_values_cat[cat_learn].append(p)\n",
    "            protected_p_values[j] = protected_p\n",
    "            protected_p_values_cat[cat_learn].append(protected_p)\n",
    "            martingale.update_martingale_value(p)\n",
    "            calibration_martingale.update_martingale_value(protected_p)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd = dists[144]\n",
    "cpd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpd = dists[144]\n",
    "\n",
    "# yrange = np.linspace(0, 50, num=1000, endpoint=True)\n",
    "# plt.plot(\n",
    "#     yrange,\n",
    "#     [cpd(y=y, tau=0) for y in yrange],\n",
    "#     label=r'$\\Pi(y, 0)$'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     yrange,\n",
    "#     [cpd(y=y, tau=1) for y in yrange],\n",
    "#     label=r'$\\Pi(y, 1)$'\n",
    "# )\n",
    "\n",
    "# plt.fill_between(\n",
    "#     yrange,\n",
    "#     [cpd(y=y, tau=0) for y in yrange],\n",
    "#     [cpd(y=y, tau=1) for y in yrange],\n",
    "#     alpha=0.7,\n",
    "#     color='C2'\n",
    "# )\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "base_log_loss = log_loss(y_true=(res[:, 2] <= limit).astype(int), y_pred=res[:, 0])\n",
    "protected_log_loss = log_loss(y_true=(res[:, 2] <= limit).astype(int), y_pred=res[:, 1])\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "base_brier_score_loss = brier_score_loss(y_true=(res[:, 2] <= limit).astype(int), y_proba=res[:, 0])\n",
    "protected_brier_score_loss = brier_score_loss(y_true=(res[:, 2] <= limit).astype(int), y_proba=res[:, 1])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "base_roc_auc_score = roc_auc_score(y_true=(res[:, 2] <= limit).astype(int), y_score=res[:, 0])\n",
    "protected_roc_auc_score = roc_auc_score(y_true=(res[:, 2] <= limit).astype(int), y_score=res[:, 1])\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "base_average_precision_score = average_precision_score(y_true=(res[:, 2] <= limit).astype(int), y_score=res[:, 0])\n",
    "protected_average_precision_score = average_precision_score(y_true=(res[:, 2] <= limit).astype(int), y_score=res[:, 1])\n",
    "\n",
    "losses = np.array(\n",
    "    [\n",
    "        [base_log_loss, base_brier_score_loss, base_roc_auc_score, base_average_precision_score],\n",
    "        [protected_log_loss, protected_brier_score_loss, protected_roc_auc_score, protected_average_precision_score]\n",
    "    ]\n",
    ")\n",
    "df_losses = pd.DataFrame(data=losses, index=['base', 'protected'], columns=['log_loss', 'brier_score', 'roc_auc', 'pr_auc'])\n",
    "df_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "base_warnings = (res[:, 0] < confidence).sum()\n",
    "protected_warnings = (res[:, 1] < confidence).sum()\n",
    "print(f'Base warnigns at confidence {confidence}: {base_warnings}')\n",
    "print(f'Protected warnigns at confidence {confidence}: {protected_warnings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res[:, 0])\n",
    "plt.plot(res[:, 1], alpha=0.5)\n",
    "plt.axhline(0.95, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 13\n",
    "plt.hist(p_values_cat[h], density=True)\n",
    "plt.hist(protected_p_values_cat[h], density=True, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(martingale.log10_martingale_values)\n",
    "plt.plot(calibration_martingale.log10_martingale_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppers = np.array([dists[i].quantile(0.95, tau) for i, tau in zip(dists.keys(), res[:, 3])])\n",
    "(uppers < res[:, 2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles = {}\n",
    "\n",
    "# for quantile in tqdm(np.linspace(0.01, 1, 100, endpoint=False)):\n",
    "\n",
    "#     quantile = np.round(quantile, 2)\n",
    "\n",
    "#     uppers = np.array([dists[i].quantile(quantile, tau) for i, tau in zip(dists.keys(), res[:, 3])])\n",
    "\n",
    "#     Quantiles[quantile] = uppers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_martingale = PluginMartingale(warnings=False)\n",
    "for p in p_values:\n",
    "    post_martingale.update_martingale_value(p)\n",
    "\n",
    "prange = np.linspace(0, 1, 1000)\n",
    "plt.plot(\n",
    "    prange,\n",
    "    post_martingale.B_n(prange),\n",
    "    label='base'\n",
    ")\n",
    "plt.plot(\n",
    "    prange,\n",
    "    calibration_martingale.B_n(prange),\n",
    "    label='protected'\n",
    ")\n",
    "plt.plot(\n",
    "    prange,\n",
    "    prange,\n",
    "    label='perfect',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_values, density=True)\n",
    "plt.hist(protected_p_values, density=True, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use CPD for decision making?\n",
    "Let $\\Pi$ be the CPD, and say we have a decision space $D$. Then we could take\n",
    "$$\\hat{d} = \\text{argmax}_d \\int_{-\\infty}^{\\infty}U(d,y)\\Pi(dy)$$\n",
    "as the decision. Now we just have to figure out how to compute this quantity for a decision, and then figure out if it is possible to do it for all decisions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(flex, limit, demand, flex_cost=0.1, overdraw_cost=1):\n",
    "    '''\n",
    "    Simple utility function that takes into account the possibility to buy too little flex.\n",
    "    NOTE: This is unrealistic in that we can not buy a real number of flex in practice.\n",
    "    '''\n",
    "    effective_demand = demand - flex # Are we above despide having flex?\n",
    "    effective_above = effective_demand > limit \n",
    "    above = demand > limit # Would we have been above if we did not buy flex?\n",
    "    if effective_above: # Even with flex, we are above the limit\n",
    "        u = - overdraw_cost * (effective_demand - limit) - flex_cost * flex\n",
    "    elif above: # We would have been over the limit if we had not had flex.\n",
    "        u = overdraw_cost * (demand - limit) - flex_cost * flex\n",
    "    else: # We are below no matter what.\n",
    "        u = - flex_cost * flex\n",
    "    return float(u)\n",
    "\n",
    "def exp_util(cpd, d, tau=1):\n",
    "    jump_points = np.unique(cpd.Y[1:-1])\n",
    "    Delta_Q = np.array([utility(d, limit, y) * (cpd(y=y + MACHINE_EPSILON(y), tau=tau) - cpd(y=y - MACHINE_EPSILON(y), tau=tau)) for y in jump_points])\n",
    "    return jump_points @ Delta_Q\n",
    "\n",
    "def expectation(cpd, tau=1):\n",
    "    jump_points = np.unique(cpd.Y[1:-1])\n",
    "    Delta_Q = np.array([(cpd(y=y + MACHINE_EPSILON(y), tau=tau) - cpd(y=y - MACHINE_EPSILON(y), tau=tau)) for y in jump_points])\n",
    "    return jump_points @ Delta_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(res[:, 2] > 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 154\n",
    "cpd = dists[i]\n",
    "plt.step(cpd.Y[:], (1 - tau) * cpd.L + tau * cpd.U, label=r'$\\Pi(y, \\tau)$')\n",
    "res[i, 2], cpd(limit, 0), res[i, 0]\n",
    "plt.axvline(res[i, 2], color='red', linestyle='--', label='truth')\n",
    "plt.axvline(cpd.quantile(0.9, tau=tau), color='C1', label='0.9 quantile')\n",
    "plt.axvline(expectation(cpd, tau), linestyle='--', label='expectation', color='C2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrange = np.linspace(0.01, 1, endpoint=False, num=99)\n",
    "plt.plot(\n",
    "    qrange,\n",
    "    [cpd.quantile(q, tau) for q in qrange],\n",
    "    label='quantile function'\n",
    ")\n",
    "plt.axhline(res[i, 2], linestyle='--', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This maximisation of expected utility is quite crude. \n",
    "A better way may be to fit a CPS to a decision, as in the working paper. However, this is computatinally expensive, and absolutely requires discrete decisions.\n",
    "\n",
    "Since we are anyway going to need to discretise the decision space, perhaps this is a reasonable way to go. Perhaps we need to speak to the experts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "tau = rnd_gen.uniform(0, 1)\n",
    "d = minimize_scalar(lambda x: -exp_util(cpd, x, tau), bounds=(0, 10)).x\n",
    "print()\n",
    "print(f'Decicion: {d}')\n",
    "print(f'Optimal decision: {max(res[i, 2] - limit, 0)}')\n",
    "print(f'Expected utility: {exp_util(cpd, d, tau)}')\n",
    "print(f'Utility {utility(d, limit, res[i, 2])}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drange = np.linspace(0, 10, num=1000, endpoint=True)\n",
    "plt.plot(\n",
    "    drange,\n",
    "    [exp_util(cpd, d, tau) for d in drange]\n",
    ")\n",
    "plt.plot(\n",
    "    drange,\n",
    "    [utility(d, limit, res[i, 2]) for d in drange]\n",
    ")\n",
    "plt.axvline(max(res[i, 2] - limit, 0), linestyle='--', color='red', label='optimal decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_util(cpd, d, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility(d, limit, res[i, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet another alternative is to simply buy if the predicted probability of being below is \"too small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd(limit, tau), cpd.quantile(0.9, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another decision idea:\n",
    "Comming back to an early idea; if we use CPS to warn, which it seems to do quite well, we can then train a forecaster on data that historically exceeded the limit, thus disregarding all old examples that are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in most cases, the demand 24 hours before was quite high in previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[data.demand_24 > limit].demand.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about; if we warn, train a new CPD, based on the following training set: All examples in the past whose label exceed the limit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlineCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
